{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec32b37",
   "metadata": {},
   "source": [
    "First, we need to run the following command to load the relevant environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825191b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e730d7d",
   "metadata": {},
   "source": [
    "The first step is to collect and load your data — For this example, we will use President Biden’s State of the Union Address from 2022 as additional context. The raw text document is available in LangChain’s GitHub repository. To load the data, we can use one of LangChain’s many built-in DocumentLoaders. A Document is a dictionary with text and metadata. To load text, we will use LangChain’s TextLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc13eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt\"\n",
    "res = requests.get(url)\n",
    "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
    "    f.write(res.text)\n",
    "\n",
    "loader = TextLoader('./state_of_the_union.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd7e0b8",
   "metadata": {},
   "source": [
    "Next, chunk the documents — Because the Document, in its original state, is too long to fit into the LLM’s context window, we need to chunk it into smaller pieces. LangChain comes with many built-in text splitters for this purpose. For this simple example, we can use the CharacterTextSplitter with a chunk_size of about 500 and a chunk_overlap of 50 to preserve text continuity between the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1354298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d73a13",
   "metadata": {},
   "source": [
    "Lastly, let's embed and store the chunks — To enable semantic search across the text chunks, we need to generate the vector embeddings for each chunk and then store them together with their embeddings. To generate the vector embeddings, we can use the OpenAI embedding model, and to store them, we can use the Weaviate vector database. By calling .from_documents() the vector database is automatically populated with the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96640cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "client = weaviate.Client(\n",
    "  embedded_options = EmbeddedOptions()\n",
    ")\n",
    "\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    client = client,    \n",
    "    documents = chunks,\n",
    "    embedding = OpenAIEmbeddings(),\n",
    "    by_text = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2315dc",
   "metadata": {},
   "source": [
    "Once the vector database is populated, we can define it as the retriever component, which fetches the additional context based on the semantic similarity between the user query and the embedded chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e98f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9946ecf8",
   "metadata": {},
   "source": [
    "Next, to augment the prompt with the additional context, we need to prepare a prompt template. The prompt can be easily customized from a prompt template, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a89b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb49b6",
   "metadata": {},
   "source": [
    "Finally, we can build a chain for the RAG pipeline, chaining together the retriever, the prompt template and the LLM. Once the RAG chain is defined, we can invoke it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16385c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")\n",
    "\n",
    "query = \"What did the president say about Justice Breyer\"\n",
    "rag_chain.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
