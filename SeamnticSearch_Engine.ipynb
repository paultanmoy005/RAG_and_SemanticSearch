{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Set the environment variables to start logging traces"
      ],
      "metadata": {
        "id": "1RyMhmaWF7SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export LANGCHAIN_TRACING_V2=\"true\"\n",
        "!export LANGCHAIN_API_KEY=\"\""
      ],
      "metadata": {
        "id": "BAlXryAT6ZoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n",
        "\n",
        "1. page_content: a string representing the content;\n",
        "2. metadata: a dict containing arbitrary metadata;\n",
        "3. id: (optional) a string identifier for the document.\n",
        "The metadata attribute can capture information about the source of the document, its relationship to other documents, and other information.\n",
        "We can generate sample documents when desired."
      ],
      "metadata": {
        "id": "1y5eIzJMGIpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "ae1Hc1aV7HPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load a PDF into a sequence of Document objects. There is a sample PDF in the LangChain repo here -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for available PDF document loaders. Let's select PyPDFLoader, which is fairly lightweight."
      ],
      "metadata": {
        "id": "rJWw7DPOHznd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "file_path = \"nke-10k-2023.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "print(len(docs))"
      ],
      "metadata": {
        "id": "58Tu4N9p7wst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyPDFLoader loads one Document object per PDF page. For each, we can easily access:\n",
        "\n",
        "* The string content of the page;\n",
        "* Metadata containing the file name and page number."
      ],
      "metadata": {
        "id": "cz2yjKVuIsfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{docs[0].page_content[:200]}\\n\")\n",
        "print(docs[0].metadata)"
      ],
      "metadata": {
        "id": "DRg3pYog9yUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\n",
        "\n",
        "We can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
        "\n",
        "We set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute “start_index”."
      ],
      "metadata": {
        "id": "Q9O1ho4aJKsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "len(all_splits)"
      ],
      "metadata": {
        "id": "HbW5kVLY9y6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\n",
        "\n",
        "LangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. Let's select a model:"
      ],
      "metadata": {
        "id": "g5zAhJKhJyX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ],
      "metadata": {
        "id": "liY0fkUW97ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
        "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
        "\n",
        "assert len(vector_1) == len(vector_2)\n",
        "print(f\"Generated vectors of length {len(vector_1)}\\n\")\n",
        "print(vector_1[:10])"
      ],
      "metadata": {
        "id": "hPobkQ12-FtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\n",
        "\n",
        "LangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let's select a vector store:"
      ],
      "metadata": {
        "id": "K2hpZUsDLFwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "metadata": {
        "id": "xPYU-vLrLKN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having instantiated our vector store, we can now index the documents."
      ],
      "metadata": {
        "id": "Bl6U4K33LtiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = vector_store.add_documents(documents=all_splits)"
      ],
      "metadata": {
        "id": "aiAF53vzLxln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\n",
        "\n",
        "Return documents based on similarity to a string query:"
      ],
      "metadata": {
        "id": "IhSGKioXL1DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search(\n",
        "    \"How many distribution centers does Nike have in the US?\"\n",
        ")\n",
        "\n",
        "print(results[0])"
      ],
      "metadata": {
        "id": "rawKV7ypMLUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Async query:"
      ],
      "metadata": {
        "id": "VvEBDwb8NFjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = await vector_store.asimilarity_search(\"When was Nike incorporated?\")\n",
        "\n",
        "print(results[0])"
      ],
      "metadata": {
        "id": "gK26iWevNI_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}